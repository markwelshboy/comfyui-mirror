#!/usr/bin/env bash
# ======================================================================
# ComfyUI Vast-AI Environment Configuration
# ======================================================================

# ----- Core -----
export DEBIAN_FRONTEND=noninteractive
export PATH="/opt/venv/bin:$PATH"
export PY="/opt/venv/bin/python"
export PIP="/opt/venv/bin/pip"

# ----- Project Paths -----
export REPO_ROOT="/workspace/comfyui-mirror"
export COMFY_HOME="/workspace/ComfyUI"
export COMFY=$COMFY_HOME
export COMFYUI_PATH=$COMFY_HOME
export COMFY_LOGS="/workspace/logs"
export CUSTOM_DIR="$COMFY_HOME/custom_nodes"
export CUSTOM_LOG_DIR="${COMFY_LOGS}/custom_nodes"
export CACHE_DIR="$COMFY_HOME/cache"
export OUTPUT_DIR="$COMFY_HOME/output"
export BUNDLES_DIR="$COMFY_HOME/bundles"

# ----- Python / pip -----
export PIP_NO_INPUT=1
export PIP_DISABLE_PIP_VERSION_CHECK=1
export PIP_NO_CACHE_DIR=1
export NUMPY_VER="numpy>=2.2,<2.3"
export CUPY_VER="cupy-cuda12x>=13.6.0"
export OPENCV_VER="opencv-contrib-python==4.12.0.88"
cat > /workspace/pins.txt <<EOF
${NUMPY_VER}
${CUPY_VER}
${OPENCV_VER}
EOF
export PIP_CONSTRAINT=/workspace/pins.txt

# ----- Git repo URL for ComfyUI / Hearmeman24 -----
export REPO_URL="https://github.com/comfyanonymous/ComfyUI"
export HEARMEMAN_REPO="https://github.com/Hearmeman24/comfyui-wan.git"

# ----- Hugging Face (mirror storage) -----
export HF_REPO_ID="markwelshboyx/hearmemanAI-comfyUI-workflows"
export HF_REPO_TYPE="${HF_REPO_TYPE:-dataset}" # or model
export CN_BRANCH="${CN_BRANCH:-main}"
export HF_TOKEN="${HF_TOKEN:-}"   # set manually or inject from Vast
export HF_API_BASE="https://huggingface.co"

# Logical set name; controls which family of bundles we fetch/publish
export BUNDLE_TAG="${BUNDLE_TAG:-Wan2_1__Wan2_2__CUDA_12_8}"

# ----- GPU Detection -----
# Pull name(s) from nvidia-smi and classify
GPU_NAME="$(nvidia-smi --query-gpu=name --format=csv,noheader | head -n1 | tr '[:space:]' '_')"
export GPU_NAME
export GPU_COUNT="$(nvidia-smi -L | wc -l)"

case "$GPU_NAME" in
  *L40S*|*L4*|*RTX_6000_ADA*)
      export GPU_ARCH="sm_89"
      export SAGE_COMMIT="68de379"          # proven stable on Ada/L40S
      export TORCH_INDEX="https://download.pytorch.org/whl/nightly/cu128"
      ;;
  *A100*|*H100*|*A800*|*H800*)
      export GPU_ARCH="sm_90"
      export SAGE_COMMIT="68de379"          # still valid; performs well
      export TORCH_INDEX="https://download.pytorch.org/whl/nightly/cu128"
      ;;
  *3090*|*4090*|*A40*|*A5000*)
      export GPU_ARCH="sm_86"
      export SAGE_COMMIT="68de379"
      export TORCH_INDEX="https://download.pytorch.org/whl/nightly/cu121"
      ;;
  *)
      export GPU_ARCH="sm_89"
      export SAGE_COMMIT="68de379"
      export TORCH_INDEX="https://download.pytorch.org/whl/nightly/cu128"
      ;;
esac

# ----- SageAttention build flags -----
export NVCC_APPEND_FLAGS="--threads 8"
export EXT_PARALLEL=4
export MAX_JOBS=32

# ----- CUDA tuning -----
export CUDA_VISIBLE_DEVICES="${CUDA_VISIBLE_DEVICES:-0}"
export TORCH_CUDA_ARCH_LIST="$GPU_ARCH"
export LD_LIBRARY_PATH="/usr/local/cuda/lib64:$LD_LIBRARY_PATH"

# ----- Ports -----
export PORT_MAIN=8188
export PORT_GPU0=8288
export PORT_GPU1=8388
export PORT_JUPYTER=8888

# ----- Misc -----
export TMUX_TMPDIR="/tmp/tmux"
export BASH_ENV="$REPO_ROOT/helpers.sh"    # auto-load helper functions
export COMFY_ENV_TAG="$(date +%Y%m%d_%H%M)_${GPU_NAME}_${GPU_COUNT}GPU"
export LOGFILE_BOOTSTRAP="/workspace/bootstrap_run.log"
export MAX_NODE_JOBS="${MAX_NODE_JOBS:-16}"         # parallelism cap
export GIT_DEPTH="${GIT_DEPTH:-1}"                 # 0 means full; 1 is shallow

# ----- Convenience echo -----
echo "ðŸ§  ComfyUI Environment Loaded"
echo "GPU: $GPU_NAME ($GPU_COUNT GPU[s])"
echo "SageAttention commit: $SAGE_COMMIT"
echo "Torch index: $TORCH_INDEX"
echo "COMFY_HOME: $COMFY_HOME"